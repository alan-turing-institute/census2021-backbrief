
@article{murray_probabilistic_2016,
	title = {Probabilistic {Record} {Linkage} and {Deduplication} after {Indexing}, {Blocking}, and {Filtering}},
	url = {http://arxiv.org/abs/1603.07816},
	abstract = {Probabilistic record linkage, the task of merging two or more databases in the absence of a unique identiﬁer, is a perennial and challenging problem. It is closely related to the problem of deduplicating a single database, which can be cast as linking a single database against itself. In both cases the number of possible links grows rapidly in the size of the databases under consideration, and in most applications it is necessary to ﬁrst reduce the number of record pairs that will be compared.},
	language = {en},
	urldate = {2019-10-09},
	journal = {arXiv:1603.07816 [stat]},
	author = {Murray, Jared S.},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.07816},
	keywords = {Statistics - Methodology},
	file = {Murray - 2016 - Probabilistic Record Linkage and Deduplication aft.pdf:/Users/echalstrey/Zotero/storage/L35CAC4A/Murray - 2016 - Probabilistic Record Linkage and Deduplication aft.pdf:application/pdf}
}

@article{christen_survey_2012,
	title = {A {Survey} of {Indexing} {Techniques} for {Scalable} {Record} {Linkage} and {Deduplication}},
	volume = {24},
	issn = {1041-4347},
	url = {http://ieeexplore.ieee.org/document/5887335/},
	doi = {10.1109/TKDE.2011.127},
	abstract = {Record linkage is the process of matching records from several databases that refer to the same entities. When applied on a single database, this process is known as deduplication. Increasingly, matched data are becoming important in many application areas, because they can contain information that is not available otherwise, or that is too costly to acquire. Removing duplicate records in a single database is a crucial step in the data cleaning process, because duplicates can severely inﬂuence the outcomes of any subsequent data processing or data mining. With the increasing size of today’s databases, the complexity of the matching process becomes one of the major challenges for record linkage and deduplication. In recent years, various indexing techniques have been developed for record linkage and deduplication. They are aimed at reducing the number of record pairs to be compared in the matching process by removing obvious non-matching pairs, while at the same time maintaining high matching quality. This paper presents a survey of twelve variations of six indexing techniques. Their complexity is analysed, and their performance and scalability is evaluated within an experimental framework using both synthetic and real data sets. No such detailed survey has so far been published.},
	language = {en},
	number = {9},
	urldate = {2019-10-09},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Christen, Peter},
	month = sep,
	year = {2012},
	pages = {1537--1555},
	file = {Christen - 2012 - A Survey of Indexing Techniques for Scalable Recor.pdf:/Users/echalstrey/Zotero/storage/TK5TUIM6/Christen - 2012 - A Survey of Indexing Techniques for Scalable Recor.pdf:application/pdf}
}

@article{elmagarmid_duplicate_2007,
	title = {Duplicate {Record} {Detection}: {A} {Survey}},
	volume = {19},
	issn = {1041-4347},
	shorttitle = {Duplicate {Record} {Detection}},
	url = {http://ieeexplore.ieee.org/document/4016511/},
	doi = {10.1109/TKDE.2007.250581},
	abstract = {Often, in the real world, entities have two or more representations in databases. Duplicate records do not share a common key and/or they contain errors that make duplicate matching a difficult task. Errors are introduced as the result of transcription errors, incomplete information, lack of standard formats, or any combination of these factors. In this paper, we present a thorough analysis of the literature on duplicate record detection. We cover similarity metrics that are commonly used to detect similar field entries, and we present an extensive set of duplicate detection algorithms that can detect approximately duplicate records in a database. We also cover multiple techniques for improving the efficiency and scalability of approximate duplicate detection algorithms. We conclude with coverage of existing tools and with a brief discussion of the big open problems in the area.},
	language = {en},
	number = {1},
	urldate = {2019-10-09},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Elmagarmid, Ahmed K. and Ipeirotis, Panagiotis G. and Verykios, Vassilios S.},
	month = jan,
	year = {2007},
	pages = {1--16},
	file = {Elmagarmid et al. - 2007 - Duplicate Record Detection A Survey.pdf:/Users/echalstrey/Zotero/storage/DEFDHFZ3/Elmagarmid et al. - 2007 - Duplicate Record Detection A Survey.pdf:application/pdf}
}

@article{yancey_evaluating_nodate,
	title = {Evaluating {String} {Comparator} {Performance} for {Record} {Linkage}},
	abstract = {We compare variations of string comparators based on the Jaro-Winkler comparator and edit distance comparator. We apply the comparators to Census data to see which are better classiﬁers for matches and nonmatches, ﬁrst by comparing their classiﬁcation abilities using a ROC curve based analysis, then by considering a direct comparison between two candidate comparators in record linkage results.},
	language = {en},
	author = {Yancey, William E},
	pages = {42},
	file = {Yancey - Evaluating String Comparator Performance for Recor.pdf:/Users/echalstrey/Zotero/storage/W4LG3WVD/Yancey - Evaluating String Comparator Performance for Recor.pdf:application/pdf}
}

@inproceedings{wilson_beyond_2011,
	address = {San Jose, CA, USA},
	title = {Beyond probabilistic record linkage: {Using} neural networks and complex features to improve genealogical record linkage},
	isbn = {978-1-4244-9635-8},
	shorttitle = {Beyond probabilistic record linkage},
	url = {http://ieeexplore.ieee.org/document/6033192/},
	doi = {10.1109/IJCNN.2011.6033192},
	abstract = {Probabilistic record linkage has been used for many years in a variety of industries, including medical, government, private sector and research groups. The formulas used for probabilistic record linkage have been recognized by some as being equivalent to the naïve Bayes classifier. While this method can produce useful results, it is not difficult to improve accuracy by using one of a host of other machine learning or neural network algorithms. Even a simple singlelayer perceptron tends to outperform the naïve Bayes classifier—and thus traditional probabilistic record linkage methods—by a substantial margin. Furthermore, many record linkage system use simple field comparisons rather than more complex features, partially due to the limits of the probabilistic formulas they use. This paper presents an overview of probabilistic record linkage, shows how to cast it in machine learning terms, and then shows that it is equivalent to a naïve Bayes classifier. It then discusses how to use more complex features than simple field comparisons, and shows how probabilistic record linkage formulas can be modified to handle this. Finally, it demonstrates a huge improvement in accuracy through the use of neural networks and higher-level matching features, compared to traditional probabilistic record linkage on a large (80,000 pair) set of labeled pairs of genealogical records used by FamilySearch.org.},
	language = {en},
	urldate = {2019-10-09},
	booktitle = {The 2011 {International} {Joint} {Conference} on {Neural} {Networks}},
	publisher = {IEEE},
	author = {Wilson, D. Randall},
	month = jul,
	year = {2011},
	pages = {9--14},
	file = {Wilson - 2011 - Beyond probabilistic record linkage Using neural .pdf:/Users/echalstrey/Zotero/storage/69SNYZV4/Wilson - 2011 - Beyond probabilistic record linkage Using neural .pdf:application/pdf}
}